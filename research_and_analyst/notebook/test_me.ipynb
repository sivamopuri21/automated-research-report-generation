{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092aee37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1 # removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0485f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root folder added to path c:\\Projects\\automated-research-report-generation\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"project root folder added to path\",project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611ba16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_and_analyst.utils.model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbf83c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6adc3411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-01-03T14:07:30.629926Z\", \"level\": \"warning\", \"event\": \"OPENAI_API_KEY is missing from environment\"}\n",
      "{\"timestamp\": \"2026-01-03T14:07:30.629926Z\", \"level\": \"info\", \"event\": \"GOOGLE_API_KEY loaded from environment\"}\n",
      "{\"timestamp\": \"2026-01-03T14:07:30.631431Z\", \"level\": \"info\", \"event\": \"GROQ_API_KEY loaded from environment\"}\n",
      "{\"timestamp\": \"2026-01-03T14:07:30.632437Z\", \"level\": \"warning\", \"event\": \"ASTRA_DB_API_ENDPOINT is missing from environment\"}\n",
      "{\"timestamp\": \"2026-01-03T14:07:30.632437Z\", \"level\": \"warning\", \"event\": \"ASTRA_DB_APPLICATION_TOKEN is missing from environment\"}\n",
      "{\"timestamp\": \"2026-01-03T14:07:30.634110Z\", \"level\": \"warning\", \"event\": \"ASTRA_DB_KEYSPACE is missing from environment\"}\n",
      "{\"config_keys\": [\"astra_db\", \"embedding_model\", \"retriever\", \"llm\"], \"timestamp\": \"2026-01-03T14:07:30.636122Z\", \"level\": \"info\", \"event\": \"YAML config loaded\"}\n"
     ]
    }
   ],
   "source": [
    "model_loader = ModelLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a6cc2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"provider\": \"google\", \"model\": \"gemini-2.0-flash\", \"timestamp\": \"2026-01-03T14:07:32.895071Z\", \"level\": \"info\", \"event\": \"Loading LLM\"}\n"
     ]
    }
   ],
   "source": [
    "llm = model_loader.load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd979642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59617007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
      "Please retry in 12.457821982s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 10.361878219s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 10\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1334\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1330\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1331\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1441\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1416\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1427\u001b[39m     **kwargs: Any,\n\u001b[32m   1428\u001b[39m ) -> ChatResult:\n\u001b[32m   1429\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1430\u001b[39m         messages,\n\u001b[32m   1431\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1439\u001b[39m         **kwargs,\n\u001b[32m   1440\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:231\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    224\u001b[39m params = (\n\u001b[32m    225\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.14-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.14-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:222\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:206\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\automated-research-report-generation\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\nPlease retry in 10.361878219s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 10\n}\n]"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7393da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5912e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyst(BaseModel):\n",
    "    name: str = Field(description=\"Name of the analyst\")\n",
    "    role: str = Field(description=\"Role of the analyst\")\n",
    "    affiliation: str = Field(description=\"Affiliation of the analyst\")\n",
    "    description: str = Field(description=\"Description of the analyst's research interests\")\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name:{self.name}\\n Role: {self.role} \\n Affliation: {self.affiliation} \\n Description: {self.description} \\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528197c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = Analyst(  \n",
    "    name=\"Dr. Siva Kumar\",\n",
    "    role=\"Data Scientist\",\n",
    "    affiliation=\"IIT Madras\",\n",
    "    description=\"Focuses on predictive models for healthcare data.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfbddf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:Dr. Siva Kumar\n",
      " Role: Data Scientist \n",
      " Affliation: IIT Madras \n",
      " Description: Focuses on predictive models for healthcare data. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analyst.persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82355446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(name='Dr. Priya Sharma', role='Policy Analyst', affiliation='MInistry of hralth', description='Investigates AI Policy compliance frameworks.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analyist belongs to health industry\n",
    "Analyst(\n",
    "    name=\"Dr. Neha Patel\",\n",
    "    role=\"Medical Data Scientist\",\n",
    "    affiliation=\"Standford UNiversity\",\n",
    "    description=\"Focuses on predictive models for patient outcomes.\"\n",
    "),\n",
    "Analyst(\n",
    "    name=\"Dr. Arun Varma\",\n",
    "    role=\"Ethics Researcher\",\n",
    "    affiliation=\"WHO\",\n",
    "    description=\"Explore ethical implications of AI in diagnostics\"\n",
    "),\n",
    "Analyst(\n",
    "    name=\"Dr. Priya Sharma\",\n",
    "    role=\"Policy Analyst\",\n",
    "    affiliation=\"MInistry of hralth\",\n",
    "    description=\"Investigates AI Policy compliance frameworks.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14050018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persprective(BaseModel):\n",
    "    analysts: List[Analyst] = Field(description=\"comprehensive list of analysts with their roles and affliations.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769cc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnlystsState(TypedDict):\n",
    "    topic:str\n",
    "    max_analyst:int\n",
    "    human_analyst_feedback: str\n",
    "    analysts: List[Analyst]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b952c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'finance',\n",
       " 'max_analyst': 5,\n",
       " 'human_analyst_feedback': 'give the real infof'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateAnlystsState(\n",
    "    topic=\"finance\",\n",
    "    max_analyst=5,\n",
    "    human_analyst_feedback=\"give the real infof\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92af7cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(name='Dr. Neha Patel', role='Medical Data Scientist', affiliation='Standford UNiversity', description='Focuses on predictive models for patient outcomes.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analyst(\n",
    "    name=\"Dr. Neha Patel\",\n",
    "    role=\"Medical Data Scientist\",\n",
    "    affiliation=\"Standford UNiversity\",\n",
    "    description=\"Focuses on predictive models for patient outcomes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b3f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_instructions=\"\"\"\n",
    "You are tasked with creating a set of AI analyst personas. Follow these instructions carefully.\n",
    "1. First , review the research topic:\n",
    "{topic}\n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts:\n",
    "{human_analyst_feedback}\n",
    "3. Determine the most intresting themes based upon documents and /or feedback above.\n",
    "4. Pick the top {max_analyst} themes.\n",
    "5. Assign one analyst to each theme.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf12183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nYou are tasked with creating a set of AI analyst personas. Follow these instructions carefully.\\n1. First , review the research topic:\\neducation\\n2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts:\\ngive the real infof\\n3. Determine the most intresting themes based upon documents and /or feedback above.\\n4. Pick the top 5 themes.\\n5. Assign one analyst to each theme.\\n', 'Generate the set of analysts.']\n"
     ]
    }
   ],
   "source": [
    "print([analyst_instructions.format(\n",
    "    topic=\"education\",\n",
    "    max_analyst=5,\n",
    "    human_analyst_feedback=\"give the real infof\"\n",
    ")] +[\"Generate the set of analysts.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a19bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analyst(state: GenerateAnlystsState):\n",
    "    \"\"\"It is creating my analyst\"\"\"\n",
    "    topic=state[\"topic\"]\n",
    "    max_analyst = state[\"max_analyst\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\",\"\")\n",
    "    strctured_llm = llm.with_structured_output(Persprective)\n",
    "    system_messages = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        max_analyst=max_analyst,\n",
    "        human_analyst_feedback=human_analyst_feedback\n",
    "    )\n",
    "    analysts = strctured_llm.invoke([SystemMessage(content=system_messages),HumanMessage(content=\"Generate the set of analysts.\")])\n",
    "    return {\"analysts\": analysts.analysts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "464182d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(name='Dr. Emily Carter', role='Lead Epidemiologist', affiliation='World Health Organization (WHO)', description='Specializes in global health trends, infectious disease modeling, and public health interventions.'),\n",
       "  Analyst(name='Dr. James Lee', role='Healthcare Policy Analyst', affiliation='Center for Health Policy Research', description='Focuses on healthcare policy, health economics, and healthcare access and affordability.'),\n",
       "  Analyst(name='Dr. Maria Rodriguez', role='Medical Technology Innovator', affiliation='National Institutes of Health (NIH)', description='Dedicated to medical technology, digital health, and the development of innovative diagnostic and therapeutic tools.'),\n",
       "  Analyst(name='Dr. David Chen', role='Mental Health Specialist', affiliation='American Psychiatric Association', description='Expertise in mental health, behavioral health, and the impact of social determinants on mental well-being.'),\n",
       "  Analyst(name='Dr. Sarah Brown', role='Nutrition and Wellness Expert', affiliation='Academy of Nutrition and Dietetics', description='Focuses on nutrition, wellness, and the role of diet in preventing and managing chronic diseases.')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_analyst({\n",
    "    \"topic\":\"health\",\n",
    "    \"max_analyst\":5,\n",
    "    \"human_analyst_feedback\":\"give the real infof\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e8a66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feeback(state):\n",
    "    \"\"\" feedback\"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state):\n",
    "    \"\"\"ArithmeticErrorstate\"\"\"\n",
    "    feedback = (state.get(\"human_analyst_feedback\") or \"\").strip().lower()\n",
    "    if feedback and feedback not in [\"\",\"none\",\"skip\",\"done\",\"continue\"]:\n",
    "        return \"create_analyst\"\n",
    "    return END\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69de71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b830479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eca1599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(GenerateAnlystsState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a1a06ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23fffa53f50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"create_analyst\",create_analyst)\n",
    "builder.add_node(\"human_feeback\", human_feeback)\n",
    "builder.add_edge(START, \"create_analyst\")\n",
    "builder.add_edge(\"create_analyst\", \"human_feeback\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feeback\",should_continue,\n",
    "    [\"create_analyst\",END]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac4d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a1272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(interrupt_before=[\"human_feeback\"], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8bacf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d507b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"the benifit of adopting Langgraph as an agent framework\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9d525df",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_analyst=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44d21845",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "465e9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before if block\n",
      "before if block\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"topic\": topic, \"max_analyst\": max_analyst}, thread, stream_mode=\"values\"):\n",
    "    analysts1 = event.get(\"analyst\",\"\")\n",
    "    print(\"before if block\")\n",
    "    if analysts1:\n",
    "        for analyst in analysts1:\n",
    "            print(\"in for loop\")\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e97f299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f1ceab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a071f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7fcb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5243a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - GeeksforGeeks',\n",
       "  'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
       "  'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions.',\n",
       "  'score': 0.9294982},\n",
       " {'title': 'LangGraph overview - Docs by LangChain',\n",
       "  'url': 'https://docs.langchain.com/oss/python/langgraph/overview',\n",
       "  'content': 'Trusted by companies shaping the future of agents including Klarna, Replit, Elastic, and more LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. We will commonly use LangChain components throughout',\n",
       "  'score': 0.9281033},\n",
       " {'title': \"A Developer's Guide to LangGraph for LLM Applications\",\n",
       "  'url': 'https://www.metacto.com/blogs/a-developer-s-guide-to-langgraph-building-stateful-controllable-llm-applications',\n",
       "  'content': 'At its core, LangGraph is a controllable cognitive architecture for any task. It is a library built to help developers create agentic and multi-agent applications with LLMs, by modeling workflows as a graph. This graph-based approach allows for cyclical processes, giving developers explicit control over the flow of logic. This is a significant departure from the more linear, directed acyclic graphs (DAGs) often found in other data orchestration frameworks. [...] LangGraph is a stateful, orchestration framework designed to build powerful and controllable LLM applications. It provides the essential building blocks for creating agentic systems that are reliable, customizable, and ready for production. Whether you are experimenting with multi-agent workflows or building a complex task automation tool, LangGraph offers a flexible foundation. [...] 1. LangGraph (Open Source): A free-to-use, MIT-licensed open-source library with SDKs in Python and JavaScript. It is a stateful orchestration framework that you can self-host and manage, giving you complete control over your environment.',\n",
       "  'score': 0.92604345},\n",
       " {'title': 'What is LangGraph? - IBM',\n",
       "  'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "  'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\nLLM applications: By using LangGraphs capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.',\n",
       "  'score': 0.9184877},\n",
       " {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n",
       "  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'content': \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] # LangGraph Tutorial: What Is LangGraph and How to Use It?\\n\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\n\\nJun 26, 2024   12 min read [...] For applications requiring autonomous decision-making, LangGraph enables the creation of agents that can perform tasks independently based on user inputs and predefined logic.\\n\\nThese agents can execute complex workflows, interact with other systems, and adapt to new information dynamically. LangGraph's structured framework ensures that each agent operates efficiently and effectively, making it suitable for tasks like automated customer support, data processing, and system monitoring.\",\n",
       "  'score': 0.91399205}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_search.invoke(\"langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2aaf3453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "== History ==\n",
      "LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Langgraph\", load_max_docs=2).load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f875b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-research-report-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
